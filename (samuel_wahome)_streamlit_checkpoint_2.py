# -*- coding: utf-8 -*-
"""(Samuel Wahome) Streamlit Checkpoint 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cp_fK_BtDtGrDiEUmRCIE5ERnLD3jN5w

# **Instructions**

1. Install the necessary packages
2. Import you data and perform basic data exploration phase
  - Display general information about the dataset
  - Create a pandas profiling reports to gain insights into the dataset
  - Handle Missing and corrupted values
  - Remove duplicates, if they exist
  - Handle outliers, if they exist
  - Encode categorical features
3. Based on the previous data exploration train and test a machine learning classifier
4. Create a streamlit application.
5. Add input fields for your features and a validation button at the end of the form
6. Import your ML model into the streamlit application and start making predictions given the provided features values

# **1: Load and Explore the Dataset**
## ***A. Display general information about the dataset***`
"""

import pandas as pd
from IPython.display import display, HTML

# Load the dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/Financial_inclusion_dataset.csv'
data = pd.read_csv(file_path)

# Create a copy of the original data
original_data = data.copy()

# Display general information about the dataset
info_html = data.info(buf=None)  # No need to store output as it prints directly

# Display the first few rows using HTML
head_html = data.head().to_html()

# Display summary statistics using HTML
describe_html = data.describe().to_html()

# Output the information and tables
display(HTML("<h3>Dataset Information</h3>"))
display(HTML(info_html))
display(HTML("<h3>First Few Rows of the Dataset</h3>"))
display(HTML(head_html))
display(HTML("<h3>Summary Statistics</h3>"))
display(HTML(describe_html))

"""## ***B. Create a pandas profiling reports to gain insights into the dataset***
Let's generate the pandas profiling report for more insights.
"""

!pip install ydata-profiling --upgrade

# Import necessary libraries
import pandas as pd
from ydata_profiling import ProfileReport
from google.colab import files

# Generate a profiling report to display in the notebook
profile_report = ProfileReport(data, title="Pandas Profiling Report: Electric Vehicle Data", explorative=True)
profile_report.to_notebook_iframe()

# Generate the profiling report and save to file
profile_rep_gen = ProfileReport(data, title="Pandas Profiling Report: Expresso Churn")
report_path = "/content/drive/MyDrive/Colab Notebooks/Expresso _Churn_Data_Profiling_Report.html"
profile_rep_gen.to_file(report_path)

# Display a link to view the saved HTML file
files.view(report_path)

print(f"The profiling report has been saved to: {report_path}")

"""## ***C. Find Columns with Missing Values***
The next thing is to identify and handle any missing or corrupted values in the dataset.This script does the following. We create a list of dictionaries containing information about missing values. So the script will provide a nicely formatted report of missing values
"""

import pandas as pd
import numpy as np
import io
from IPython.display import display, HTML

# Find columns with missing values
columns_with_missing = data.columns[data.isnull().any()].tolist()

# Prepare results
results = []
if columns_with_missing:
    for column in columns_with_missing:
        missing_count = data[column].isnull().sum()
        missing_percentage = (missing_count / len(data)) * 100
        results.append({
            'Column': column,
            'Missing Values': missing_count,
            'Percentage': f"{missing_percentage:.2f}%"
        })

    # Create a DataFrame for display
    missing_df = pd.DataFrame(results)

    # Display the results
    display(HTML("<h3>Columns with Missing Values:</h3>"))
    display(missing_df)
else:
    display(HTML("<p>No columns have missing values.</p>"))

# Display total number of missing values in the dataset
total_missing = data.isnull().sum().sum()
display(HTML(f"<p><strong>Total missing values in the dataset:</strong> {total_missing}</p>"))

# Display columns with no missing values
columns_without_missing = data.columns[~data.isnull().any()].tolist()
display(HTML("<h3>Columns without Missing Values:</h3>"))
display(HTML("<p>" + ", ".join(columns_without_missing) + "</p>"))

# Optional: Display overall dataset info
display(HTML("<h3>Dataset Info:</h3>"))
buffer = io.StringIO()
data.info(buf=buffer)
info_string = buffer.getvalue()
display(HTML("<pre>" + info_string + "</pre>"))

"""### **Handling Missing Values**"""

import pandas as pd
import numpy as np
import io
from IPython.display import display, HTML

display(HTML("<p>No columns have missing values.</p>"))

"""## ***D. Handle Outliers***
Next, we'll check for outliers in the numerical columns using the interquartile range (IQR) method and handle them if necessary.

**Methodology:**
1. Numeric columns: Numeric columns of the dataset are identified.
2. Histograms: Histograms for each numeric column are created.
3. Q-Q plots:
4. Handling outliers: Outliers in each numeric column are handled using the 'cap' method.
5. Plotting outliers after handling: Outliers for each column are plotted after handling them to visualize the impact of the outlier handling.

### **1. Load the set & Define Helper Functions**

These functions are designed to identify, plot, and handle outliers using multiple methods.efine functions.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Functions to identify, plot, and handle outliers
def identify_outliers(cleaned_data, column):
    Q1 = cleaned_data[column].quantile(0.25)
    Q3 = cleaned_data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    iqr_outliers = cleaned_data[(cleaned_data[column] < lower_bound) | (cleaned_data[column] > upper_bound)]

    z_scores = np.abs(stats.zscore(cleaned_data[column].dropna()))
    z_score_outliers = cleaned_data[z_scores > 3]

    median = cleaned_data[column].median()
    mad = np.median(np.abs(cleaned_data[column] - median))
    modified_z_scores = 0.6745 * (cleaned_data[column] - median) / mad
    modified_z_score_outliers = cleaned_data[np.abs(modified_z_scores) > 3.5]

    return iqr_outliers, z_score_outliers, modified_z_score_outliers

def plot_outliers(cleaned_data, column):
    plt.figure(figsize=(12, 6))
    plt.scatter(cleaned_data.index, cleaned_data[column], alpha=0.5)
    iqr, z, mod_z = identify_outliers(cleaned_data, column)
    plt.scatter(iqr.index, iqr[column], color='red', label='IQR')
    plt.scatter(z.index, z[column], color='green', label='Z-score')
    plt.scatter(mod_z.index, mod_z[column], color='orange', label='Modified Z-score')
    plt.title(f'Outliers in {column}')
    plt.xlabel('Index')
    plt.ylabel(column)
    plt.legend()
    plt.show()

def handle_outliers(cleaned_data, column, method='cap'):
    _, _, outliers = identify_outliers(cleaned_data, column)
    if method == 'cap':
        lower = cleaned_data[column].quantile(0.01)
        upper = cleaned_data[column].quantile(0.99)
        cleaned_data[column] = cleaned_data[column].clip(lower, upper)
    elif method == 'remove':
        cleaned_data = cleaned_data[~cleaned_data.index.isin(outliers.index)]
    return cleaned_data

"""### **2. Exploratory Data Analysis (EDA):**
Here, we need to understand the distribution and characteristics of the data. It provides a summary of the dataset, creates histograms, and generates Q-Q plots for each numeric column. This helps in understanding the data distribution and potential outliers.

"""

import numpy as np
import pandas as pd
from IPython.display import display, HTML

# Define cleaned_data if not done already
cleaned_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Financial_inclusion_dataset.csv')

# Create a copy of cleaned_data for outlier handling
data_for_outliers = cleaned_data.copy()

# Print descriptive statistics
descriptive_stats = data_for_outliers.describe()

# Display descriptive statistics using IPython.display
display(HTML("<h3>Descriptive Statistics:</h3>"))
display(HTML(descriptive_stats.to_html()))

# Get numeric columns
numeric_columns = data_for_outliers.select_dtypes(include=[np.number]).columns
n_cols = len(numeric_columns)

print('\n')
print(f"Number of numeric columns: {n_cols}")

"""### **3. Create Visualisations**
Now we will display histograms for all numeric columns in the data_for_outliers DataFrame, showing the distribution of each column's values.

#### **1. Histograms**
"""

import matplotlib.pyplot as plt
from IPython.display import display, HTML

# Calculate number of rows and columns for subplots
n_cols = len(numeric_columns)
n_rows = (n_cols + 2) // 3
n_cols_plot = min(3, n_cols)

# Create histograms
fig, axes = plt.subplots(n_rows, n_cols_plot, figsize=(5 * n_cols_plot, 4 * n_rows))
fig.suptitle('Histograms of Numeric Columns', fontsize=16, y=1.05)  # Increase y for more space

print('\n')
for i, column in enumerate(numeric_columns):
    row = i // 3
    col = i % 3
    ax = axes[row, col] if n_rows > 1 else axes[col]
    ax.hist(data_for_outliers[column], bins=30)
    ax.set_title(column)
    ax.tick_params(axis='both', which='major', labelsize=8)

# Remove any unused subplots
for i in range(n_cols, n_rows * n_cols_plot):
    row = i // 3
    col = i % 3
    fig.delaxes(axes[row, col] if n_rows > 1 else axes[col])

# Adjust layout to prevent overlap between subplots and title
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to leave space for the title
plt.subplots_adjust(top=0.88, hspace=0.4, wspace=0.3)  # Adjust top for even more space
plt.show()

"""#### **2. Q-Plots**
Next we create Q-Q plots for each numeric column in the data_for_outliers DataFrame. Each plot will help visualize how the data deviates from a normal distribution. Q-Q plots for each numeric column are created to assess normality.
"""

import matplotlib.pyplot as plt
import scipy.stats as stats

# Calculate number of rows and columns for subplots
n_cols = len(numeric_columns)
n_rows = (n_cols + 2) // 3
n_cols_plot = min(3, n_cols)

# Create Q-Q plots
fig, axes = plt.subplots(n_rows, n_cols_plot, figsize=(5 * n_cols_plot, 4 * n_rows))
fig.suptitle('Q-Q Plots of Numeric Columns', fontsize=16, y=1.05)  # Adjust y for title space

# If axes is not a list, convert it to a list for consistent iteration
if n_rows == 1 and n_cols_plot == 1:
    axes = [axes]
elif n_rows == 1:
    axes = axes
else:
    axes = axes.flatten()  # Flatten axes array for easy iteration

# Plot Q-Q plots
for i, column in enumerate(numeric_columns):
    ax = axes[i]
    stats.probplot(data_for_outliers[column], dist="norm", plot=ax)  # Plot directly on ax
    ax.set_title(f"Q-Q Plot of {column}")
    ax.tick_params(axis='both', which='major', labelsize=8)

# Remove any unused subplots
for i in range(len(numeric_columns), len(axes)):
    fig.delaxes(axes[i])

# Adjust layout to avoid overlap
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.subplots_adjust(top=0.88, hspace=0.4, wspace=0.3)

# Display the figure only once
plt.show()

"""#### **3. Identify & Plot Outliers**
Here we identify and visualize outliers for each numeric column using multiple methods.
"""

import matplotlib.pyplot as plt
from IPython.display import display, HTML

# Loop through each numeric column to identify and plot outliers
for column in numeric_columns:
    # Identify outliers using different methods
    iqr_outliers, z_score_outliers, modified_z_score_outliers = identify_outliers(data_for_outliers, column)

    # Display the number of outliers found by each method
    outlier_info = f"""
    <h3>Outliers in {column}:</h3>
    <ul>
        <li><strong>IQR method:</strong> {len(iqr_outliers)}</li>
        <li><strong>Z-score method:</strong> {len(z_score_outliers)}</li>
        <li><strong>Modified Z-score method:</strong> {len(modified_z_score_outliers)}</li>
    </ul>
    """
    display(HTML(outlier_info))

    # Plot outliers for the column
    plt.figure(figsize=(12, 6))
    plot_outliers(data_for_outliers, column)
    plt.show()

"""#### **4. Handle Outliers & Compare Results**
**The Cap Method**

Instead of removing outliers, I decided to cap them at the 1st and 99th percentiles. It preserves data points while mitigating the impact of extreme values.
"""

import pandas as pd
from IPython.display import display, HTML

data_no_outliers = data_for_outliers.copy()

for column in numeric_columns:
    data_no_outliers = handle_outliers(data_no_outliers, column, method='cap')

# Compare shapes
shapes = [
    ["Original", data_for_outliers.shape[0], data_for_outliers.shape[1]],
    ["After handling outliers", data_no_outliers.shape[0], data_no_outliers.shape[1]]
]
shapes_df = pd.DataFrame(shapes, columns=["Dataset", "Rows", "Columns"])
display(HTML("<h3>Comparison of Shapes</h3>"))
display(HTML(shapes_df.to_html(index=False, border=0)))

# Compare summary statistics
stats_before = data_for_outliers.describe().T
stats_after = data_no_outliers.describe().T

display(HTML("<h3>Summary Statistics Before Outlier Handling</h3>"))
display(HTML(stats_before.to_html(float_format="{:.2f}".format, border=0)))

display(HTML("<h3>Summary Statistics After Outlier Handling</h3>"))
display(HTML(stats_after.to_html(float_format="{:.2f}".format, border=0)))

# Count the number of changes
changes = (data_for_outliers != data_no_outliers).sum()
changes_df = pd.DataFrame({"Column": changes.index, "Changes": changes.values})
display(HTML("<h3>Number of Values Changed in Each Column</h3>"))
display(HTML(changes_df.to_html(index=False, border=0)))

display(HTML("<p><strong>Note:</strong> The original 'cleaned_data' DataFrame remains unchanged.</p>"))
display(HTML("<p>Use 'data_no_outliers' for further analysis with outliers handled.</p>"))

"""## ***E. Encode Categorical Features:***
The Code does the following:

1. Checks and displays information about the original cleaned_data.
2. Identifies categorical columns and performs label encoding.
3. Creates a new encoded_data DataFrame with the encoded values.
4. Provides detailed diagnostic information about both the original and encoded data.
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from IPython.display import display, HTML

# Diagnostic step: Check the data after outlier handling
display(HTML("<h3>Data after Outlier Handling</h3>"))
display(HTML(f"<p>Shape: {data_no_outliers.shape}</p>"))

display(HTML("<h4>Column Names:</h4>"))
display(pd.DataFrame(data_no_outliers.columns, columns=['Column Names']))

display(HTML("<h4>Data Types:</h4>"))
display(pd.DataFrame(data_no_outliers.dtypes, columns=['Data Type']))

display(HTML("<h4>First Few Rows of Data:</h4>"))
display(data_no_outliers.head())

# Identify categorical columns
categorical_columns = data_no_outliers.select_dtypes(include=['object']).columns

# Create a new DataFrame for encoded data
encoded_data = data_no_outliers.copy()

# Use Label Encoding for categorical variables
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    encoded_data[col] = le.fit_transform(data_no_outliers[col].astype(str))
    label_encoders[col] = le

# Encoding process completion message
display(HTML("<h3>Encoding Process Complete</h3>"))
display(HTML("<p>========================</p>"))

# Display information about encoded columns
display(HTML("<h4>Categorical Columns Encoded:</h4>"))
display(pd.DataFrame(categorical_columns, columns=['Encoded Columns']))

# Display shape of dataframes
display(HTML(f"<p>Shape of data after outlier handling: {data_no_outliers.shape}</p>"))
display(HTML(f"<p>Shape of encoded data: {encoded_data.shape}</p>"))

# Diagnostic steps for encoded_data
display(HTML("<h4>Diagnostic Information for encoded_data:</h4>"))
display(HTML(f"<p>Number of rows: {len(encoded_data)}</p>"))
display(HTML(f"<p>Number of columns: {len(encoded_data.columns)}</p>"))

display(HTML("<h4>Column Names of encoded_data:</h4>"))
display(pd.DataFrame(encoded_data.columns, columns=['Column Names']))

display(HTML("<h4>Data Types of Columns in encoded_data:</h4>"))
display(pd.DataFrame(encoded_data.dtypes, columns=['Data Type']))

display(HTML("<h4>First Few Rows of Encoded Data:</h4>"))
display(encoded_data.head())

display(HTML("<h4>Sample of Encoded Data (first 5 rows, first 5 columns):</h4>"))
display(encoded_data.iloc[:5, :5])

# Check if the encoded data is empty
if encoded_data.empty:
    display(HTML("<p style='color: red;'>Warning: encoded_data is empty!</p>"))
else:
    display(HTML("<p>encoded_data contains data.</p>"))

display(HTML("<h4>Note:</h4>"))
display(HTML("<p>'data_no_outliers' remains unchanged. Use the 'encoded_data' frame for the model.</p>"))

"""#**2: Select Target Variable & the Features**


## 1. **Target Variable:**
The target variable for our analysis is `age_of_respondent`. This numeric variable represents the age of each respondent, making it the appropriate target for our regression task.

## 2. **Features:**
We have identified several features that may influence the respondent's age. These include both numeric and categorical features:

### **Numeric Features:**
- `household_size:` The number of people living in the household, which could indicate family size or socioeconomic factors.
- `year: `The year in which the survey data was collected, potentially capturing trends over time.

### **Categorical Features:**
- `region: `The geographical region where the respondent resides, which may correlate with different lifestyle or socioeconomic conditions.
- `relationship_with_head: `The relationship of the respondent to the head of the household, providing context on family roles that might align with age.
- `marital_status:` The marital status of the respondent, which is often related to age.
- `education_level:` The highest education level achieved by the respondent, which could be predictive of their age group.
- `job_type:` The type of occupation or employment the respondent is engaged in, which may vary by age.

### **Features to Potentially Exclude:**
- `unique_id:` A unique identifier for the respondent, not predictive of age.

Note: The dataset has been cleaned and encoded, and now we're ready to use these features for model training and evaluation.
"""

# Save encoded_data to CSV
encoded_data.to_csv('/content/drive/MyDrive/Colab Notebooks/finance_data_encoded_data.csv', index=False)
print("The file 'finance_data_encoded_data.csv' has been saved to the Google Drive under 'Colab Notebooks'.")

import pandas as pd
from sklearn.model_selection import train_test_split
from IPython.display import display

# Assuming encoded_data is your DataFrame after encoding

# Separate features and target variable (using encoded_data)
X = encoded_data.drop('age_of_respondent', axis=1)  # Drop the target column
y = encoded_data['age_of_respondent']  # Target column

# Check for NaN values in the target variable before splitting
nan_count_before = y.isna().sum()
print(f"Number of NaN values in 'age_of_respondent' before splitting: {nan_count_before}")

# Handle NaN values in the target variable
if nan_count_before > 0:
    encoded_data = encoded_data.dropna(subset=['age_of_respondent'])  # Drop rows where 'CHURN' is NaN
    X = encoded_data.drop('age_of_respondent', axis=1)  # Update X after dropping NaN rows
    y = encoded_data['age_of_respondent']  # Update y after dropping NaN rows

# Recheck NaN values after handling
nan_count_after = y.isna().sum()
print(f"Number of NaN values in 'age_of_respondent' after handling: {nan_count_after}")

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Recheck for NaN values in the split datasets
nan_count_y_train = y_train.isna().sum()
nan_count_y_test = y_test.isna().sum()
print(f"Number of NaN values in 'y_train': {nan_count_y_train}")
print(f"Number of NaN values in 'y_test': {nan_count_y_test}")

# Display the shapes of the datasets
print("Data Splitting Complete")
print("========================")
print(f"Training Features Shape: {X_train.shape}")
print(f"Training Target Shape: {y_train.shape}")
print(f"Testing Features Shape: {X_test.shape}")
print(f"Testing Target Shape: {y_test.shape}")

# Ensure that data types are correct for training
print("\nData Types of Training Features:")
display(pd.DataFrame(X_train.dtypes, columns=['Data Type']))

print("\nData Types of Testing Features:")
display(pd.DataFrame(X_test.dtypes, columns=['Data Type']))

print("\nFirst Few Rows of Training Features:")
display(X_train.head())

print("\nFirst Few Rows of Training Target:")
display(y_train.head())

print("\nFirst Few Rows of Testing Features:")
display(X_test.head())

print("\nFirst Few Rows of Testing Target:")
display(y_test.head())

"""# **3. Train the Model: Methodology**

## **Data Preparation**
- `prepare_data` function to handle non-numeric columns and NaN values consistently.
- Replace NaN values with the median of each column instead of 0, which is often more appropriate.

## **Infinite Value Check**
- A check for infinite values and replace them with a large finite value if found.

## **Model Parameters**
- Reduce the number of estimators to 50 to speed up training.
- Add `n_jobs=-1` to use all available cores, which can speed up training.

## **Progress Tracking**
- Display statements to show progress, which helps identify where the script might be stalling.

## **Feature Importance**
- Limited to showing only the top 20 features to reduce plotting time and memory usage.

## **Error Handling**
- The script handles non-numeric columns by dropping them, which prevents errors if any such columns remain after preprocessing.

"""

pip install imblearn

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML
from imblearn.over_sampling import SMOTE

def prepare_data(X):
    X = X.apply(pd.to_numeric, errors='coerce')
    non_numeric = X.select_dtypes(exclude=[np.number]).columns
    if len(non_numeric) > 0:
        display(HTML(f"<p>Dropping non-numeric columns: {', '.join(non_numeric)}</p>"))
        X = X.drop(columns=non_numeric)
    X = X.fillna(X.median())
    return X

# Prepare the data
X_train = prepare_data(X_train)
X_test = prepare_data(X_test)
display(HTML("<h3>Data types after preparation:</h3>"))
display(X_train.dtypes)
if np.isinf(X_train).any().any() or np.isinf(X_test).any().any():
    display(HTML("<p>Warning: Infinite values detected. Replacing with large finite values.</p>"))
    X_train = X_train.replace([np.inf, -np.inf], np.finfo(np.float64).max)
    X_test = X_test.replace([np.inf, -np.inf], np.finfo(np.float64).max)

# Handle imbalanced data using SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Initialize and train the model
rf_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
display(HTML("<p>Training the model...</p><br><br>"))
rf_model.fit(X_train, y_train)

# Make predictions
display(HTML("<p>Making predictions...</p><br><br>"))
y_pred = rf_model.predict(X_test)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
display(HTML(f"<h3>Accuracy Score: {accuracy:.4f}</h3><br><br>"))

# F1-Score (Macro)
f1 = f1_score(y_test, y_pred, average='macro')
display(HTML(f"<h3>F1-Score (Macro): {f1:.4f}</h3><br><br>"))

# Classification Report
display(HTML("<h3>Classification Report:</h3><br><br>"))
report = classification_report(y_test, y_pred, output_dict=True)
display(pd.DataFrame(report).transpose())
display(HTML("<br><br>"))

# Feature Importance
importances = rf_model.feature_importances_
features = X_train.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False).head(20)
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Top 20 Feature Importance')
display(plt.gcf())
plt.close()

display(HTML("<p>Model training and evaluation completed successfully.</p><br><br>"))

"""# **Streamlit App**"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py

! wget -q -O - ipv4.icanhazip.com

! streamlit run app.py & npx localtunnel --port 8501

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder


# Load trained model (assuming a pre-trained model is available)
def load_model(encoded_data_path):
    # Load encoded data from CSV
    df = pd.read_csv(encoded_data_path)

    X = df.drop(columns=['bank_account'])  # Feature matrix
    y = df['bank_account']  # Target variable

    # Train RandomForestClassifier for the sake of this demo
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X, y)

    return clf


# Load model
encoded_data_path = '/content/drive/MyDrive/Colab Notebooks/finance_data_encoded_data.csv'  # Replace with your path
clf = load_model(encoded_data_path)

# Streamlit app
st.title("Financial Inclusion Prediction App")

st.write("""
### Predict whether an individual has a bank account based on their characteristics.
""")

# Assuming you know the column names from your encoded DataFrame
feature_columns = ['location_type', 'cellphone_access', 'household_size', 'age_of_respondent',
                   'gender_of_respondent', 'relationship_with_head', 'marital_status', 'education_level', 'job_type']

# Create input fields for the features
location_type = st.selectbox("Location Type", feature_columns[0])
cellphone_access = st.selectbox("Cellphone Access", feature_columns[1])
household_size = st.slider("Household Size", min_value=1, max_value=20, value=3)
age_of_respondent = st.slider("Age of Respondent", min_value=16, max_value=100, value=30)
gender_of_respondent = st.selectbox("Gender", feature_columns[4])
relationship_with_head = st.selectbox("Relationship with Head", feature_columns[5])
marital_status = st.selectbox("Marital Status", feature_columns[6])
education_level = st.selectbox("Education Level", feature_columns[7])
job_type = st.selectbox("Job Type", feature_columns[8])

# Create a button for prediction
if st.button("Predict"):
    # Prepare the input data based on user selections
    input_data = pd.DataFrame({
        feature_columns[0]: [location_type],
        feature_columns[1]: [cellphone_access],
        feature_columns[2]: [household_size],
        feature_columns[3]: [age_of_respondent],
        feature_columns[4]: [gender_of_respondent],
        feature_columns[5]: [relationship_with_head],
        feature_columns[6]: [marital_status],
        feature_columns[7]: [education_level],
        feature_columns[8]: [job_type]
    })

    # Prediction using the trained model
    prediction = clf.predict(input_data)[0]

    # Define a constant for clarity
    HAS_BANK_ACCOUNT = 1

    # Check the prediction and provide feedback
    if prediction == HAS_BANK_ACCOUNT:
        st.success("The individual is predicted to have a bank account.")
    else:
        st.error("The individual is predicted to not have a bank account.")

print("Code corrected and ready for execution.")